{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##chatgpt给我的对话机器人代码，这个简单的对话机器人会不断接收用户输入，并根据用户的输入做出回应。如果用户输入\"退出\"，则机器人会回应\"再见！\"并结束对话。对于用户的其他输入，你可以根据需要自定义对话逻辑。上面的示例中，如果用户输入包含\"你好\"，机器人会回应\"你好！有什么我可以帮助你的吗？\"，如果用户输入包含\"名字\"，机器人会回应\"我是ChatGPT，一个智能对话机器人。\"如果用户输入的是其他内容，机器人会回应\"抱歉，我不明白你的意思。\"你可以根据需求扩展这个简单的对话机器人，加入更多的对话规则和逻辑。\n",
    "while True:\n",
    "    user_input = input(\"你: \")  # 接收用户输入\n",
    "    user_input = user_input.lower()  # 转换为小写字母\n",
    "    \n",
    "    if user_input == \"退出\":\n",
    "        print(\"机器人: 再见！\")\n",
    "        break\n",
    "    \n",
    "    # 编写你的对话逻辑\n",
    "    if \"你好\" in user_input:\n",
    "        print(\"机器人: 你好！有什么我可以帮助你的吗？\")\n",
    "    elif \"名字\" in user_input:\n",
    "        print(\"机器人: 我是ChatGPT，一个智能对话机器人。\")\n",
    "    else:\n",
    "        print(\"机器人: 抱歉，我不明白你的意思。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你: 你好啊\n",
      "机器人: 你好！\n",
      "你: 你叫什么\n",
      "机器人: 抱歉，我不明白你的意思。\n",
      "你: 你能做什么\n",
      "机器人: 我还不太理解，请换一种说法。\n",
      "你: 你的名字\n",
      "机器人: 我是ChatGPT，一个智能对话机器人。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 定义回复模板\n",
    "greetings = [\"你好！\", \"嗨！\", \"欢迎！\"]\n",
    "goodbyes = [\"再见！\", \"祝你有美好的一天！\", \"下次再见！\"]\n",
    "unknown_responses = [\"抱歉，我不明白你的意思。\", \"我还不太理解，请换一种说法。\"]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"你: \")\n",
    "    user_input = user_input.lower()\n",
    "    \n",
    "    if user_input == \"退出\":\n",
    "        print(\"机器人:\", random.choice(goodbyes))\n",
    "        break\n",
    "    \n",
    "    # 根据关键词匹配进行回复\n",
    "    if \"你好\" in user_input:\n",
    "        print(\"机器人:\", random.choice(greetings))\n",
    "    elif \"名字\" in user_input:\n",
    "        print(\"机器人: 我是ChatGPT，一个智能对话机器人。\")\n",
    "    else:\n",
    "        print(\"机器人:\", random.choice(unknown_responses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wenjie\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你: 你好\n",
      "机器人: 你好！有什么我可以帮助你的吗？\n",
      "你: 你的名字\n",
      "机器人: 抱歉，我不明白你的意思。\n",
      "你: 名字\n",
      "机器人: 我是一个智能对话机器人。\n",
      "你: 你\n",
      "机器人: 抱歉，我不明白你的意思。\n",
      "你: 再见\n"
     ]
    }
   ],
   "source": [
    "##关键词匹配的方法可能会受限于固定的关键词列表，因此对于复杂或变化的问题，这种方法可能不够灵活。\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 下载必要的NLTK数据\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 初始化NLTK的词形还原器和停用词\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 将文本转换为小写，并进行词形还原和停用词过滤\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # 针对用户输入的关键词进行回答生成\n",
    "    tokens = preprocess_text(user_input)\n",
    "    \n",
    "    if \"你好\" in tokens:\n",
    "        return \"你好！有什么我可以帮助你的吗？\"\n",
    "    elif \"名字\" in tokens:\n",
    "        return \"我是一个智能对话机器人。\"\n",
    "    else:\n",
    "        return \"抱歉，我不明白你的意思。\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"你: \")\n",
    "\n",
    "    if user_input == \"退出\":\n",
    "        print(\"机器人: 再见！\")\n",
    "        break\n",
    "\n",
    "    # 生成回答\n",
    "    answer = generate_response(user_input)\n",
    "    print(\"机器人:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-73fb2cb73b19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 加载预训练的GPT模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gpt2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "##想要更灵活和智能的回答，可以考虑使用自然语言处理中的语义理解和生成技术。一种常见的方法是使用基于深度学习的模型，如循环神经网络（RNN）、长短时记忆网络（LSTM）或转换器模型（如BERT、GPT）。这些模型可以学习语言的上下文和语义，从而生成更准确和自然的回答。以下是一个使用预训练的GPT模型（例如GPT-2）进行对话的示例代码，它可以根据上下文生成连贯的回答：\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# 加载预训练的GPT模型\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 设置生成回答的参数\n",
    "max_length = 50\n",
    "temperature = 0.7\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"你: \")\n",
    "\n",
    "    if user_input == \"退出\":\n",
    "        print(\"机器人: 再见！\")\n",
    "        break\n",
    "\n",
    "    # 将用户输入编码为模型可理解的格式\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "\n",
    "    # 使用GPT模型生成回答\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature)\n",
    "    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"机器人:\", response)\n",
    "    \n",
    "    ##在这个示例中，我们使用了Hugging Face的transformers库，它提供了一种方便使用预训练模型的接口。我们加载了预训练的GPT-2模型和相应的分词器，然后使用模型来生成回答。根据用户的输入，我们将其编码为模型可以理解的输入格式，并使用model.generate()函数生成回答。生成的回答会被解码为可读文本，并作为机器人的回复打印出来。\n",
    "    ##请注意，这个示例依赖于预训练的GPT-2模型，因此你需要安装transformers库并下载GPT-2模型的权重文件。这些模型通常需要较大的计算资源和较长的推理时间，但它们可以生成更灵活和智能的回答，因为它们能够捕捉更多的语义和上下文信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你: 你好\n",
      "机器人: 嗨！\n",
      "你: 天气如何\n",
      "机器人: 抱歉，我不明白你的意思。\n",
      "你: 天气\n",
      "机器人: 今天天气晴朗。\n",
      "你: 名字\n",
      "机器人: 我是ChatGPT，一个智能对话机器人。\n",
      "你: 喜欢的电影\n",
      "机器人: 我喜欢《星际穿越》。\n",
      "你: 喜欢的电影\n",
      "机器人: 《肖申克的救赎》是我的最爱。\n",
      "你: 你好\n",
      "机器人: 欢迎！\n",
      "你: 退出\n",
      "机器人: 再见！\n"
     ]
    }
   ],
   "source": [
    "##改进的关键词匹配的方法：\n",
    "#使用更多的关键词和短语：扩展关键词列表，包括更多可能的用户输入。这样可以增加匹配的准确性和覆盖范围。\n",
    "\n",
    "#引入近义词和同义词：考虑将同义词或近义词添加到关键词列表中，以便更好地匹配用户输入。可以使用词典或词库来获取同义词信息。\n",
    "\n",
    "#使用模糊匹配：在关键词匹配时，可以考虑使用模糊匹配算法，如编辑距离算法或相似度计算方法，以处理拼写错误或近似匹配的情况。\n",
    "\n",
    "#引入上下文理解：结合上下文信息，以更好地理解用户的意图。可以通过保存和分析之前的对话历史来实现上下文理解。\n",
    "\n",
    "#使用机器学习方法：考虑使用机器学习方法，如分类器或序列标注模型，来训练一个模型，将用户输入映射到合适的回答。这样可以更好地处理复杂的语义关系。\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 下载必要的NLTK数据\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 初始化NLTK的词形还原器和停用词\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 定义问题-回答映射字典\n",
    "question_responses = {\n",
    "    \"你好\": [\"你好！\", \"嗨！\", \"欢迎！\"],\n",
    "    \"名字\": [\"我是ChatGPT，一个智能对话机器人。\"],\n",
    "    \"天气\": [\"今天天气晴朗。\", \"明天会下雨。\"],\n",
    "    \"喜欢的电影\": [\"我喜欢《星际穿越》。\", \"《肖申克的救赎》是我的最爱。\"],\n",
    "    # 添加更多问题和回答\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 将文本转换为小写，并进行词形还原和停用词过滤\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # 针对用户输入的关键词进行回答生成\n",
    "    tokens = preprocess_text(user_input)\n",
    "    \n",
    "    for keyword in question_responses:\n",
    "        if keyword in tokens:\n",
    "            return random.choice(question_responses[keyword])\n",
    "    \n",
    "    return \"抱歉，我不明白你的意思。\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"你: \")\n",
    "\n",
    "    if user_input == \"退出\":\n",
    "        print(\"机器人: 再见！\")\n",
    "        break\n",
    "\n",
    "    # 生成回答\n",
    "    answer = generate_response(user_input)\n",
    "    print(\"机器人:\", answer)\n",
    "\n",
    "    \n",
    "#在这个改进的代码中，我们使用了一个问题-回答映射字典question_responses，其中包含了一些常见问题和对应的回答。我们还添加了上下文理解的能力，通过对用户输入进行预处理和关键词匹配来选择合适的回答。如果用户输入的问题在映射字典中有匹配项，就会返回对应的回答。如果没有匹配项，就会返回默认的\"抱歉，我不明白你的意思。\"回答。\n",
    "\n",
    "#这样改进后的代码更加灵活，可以根据问题的关键词进行回答，同时可以扩展和定制问题-回答的映射字典，以满足更多的对话需求。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-2ff048c2a640>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2ff048c2a640>\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# 1. 加载预训练模型和分词器\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "\n",
    "# 2. 将用户输入转换为词向量\n",
    "def convert_input_to_token(input_text):\n",
    "    return tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 3. 将词向量输入 GPT 模型并生成文本输出\n",
    "def generate_text(input_token, max_length=50):\n",
    "    output = model.generate(input_token, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 4. 将 GPT 的文本输出转换为自然语言\n",
    "def convert_token_to_text(output_token):\n",
    "    return tokenizer.decode(output_token, skip_special_tokens=True)\n",
    "\n",
    "# 5. 与用户进行交互\n",
    "def interact():\n",
    "    print(\"请输入您想要与模型对话的内容：\")\n",
    "    input_text = input()\n",
    "    input_token = convert_input_to_token(input_text)\n",
    "    output_token = generate_text(input_token)\n",
    "    output_text = convert_token_to_text(output_token)\n",
    "    print(\"模型回复：\", output_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    interact()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's society is a rapidly advancing one, with various technologies constantly being introduced, making people dizzy.\n",
      "\n",
      "But the problem is that the technology is not being used to solve problems.\n",
      "\n",
      "The problem is that the technology is not being used to solve problems.\n",
      "\n",
      "The problem is that the technology is not being used to solve problems.\n",
      "\n",
      "The problem is that the technology is not being used to solve problems.\n",
      "\n",
      "The problem is that the technology is not being used to solve\n"
     ]
    }
   ],
   "source": [
    "#最简陋的一个语言模型，功能是文本生成，采用的是transformers和torch库\n",
    "# 导入torch库\n",
    "import torch\n",
    "# 导入transformers库中的GPT2Tokenizer和GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 设置预训练模型名称为\"gpt2\"\n",
    "model_name = \"gpt2\"\n",
    "# 加载预训练模型和分词器，根据上面的模型名称的\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# 输入文本\n",
    "text = \"Today's society is a rapidly advancing one, with various technologies constantly being introduced, making people dizzy.\"    #在此输入文本，shift+enter运行代码\n",
    "# 使用分词器对输入文本进行编码，并返回输入ID\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "###生成文本：或文本生成\n",
    "output = model.generate(input_ids, max_length=100)    #控制输出token的字符个数\n",
    "# 将生成的输出解码为文本\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 输出结果\n",
    "print(decoded_output)\n",
    "\n",
    "\n",
    "#这段代码主要使用了PyTorch库和transformers库中的GPT2模型来进行文本生成。\n",
    "\n",
    "#整段代码的实现思路是：\n",
    "#导入所需的库，包括torch和transformers。\n",
    "#设置预训练模型名称为\"gpt2\"。\n",
    "#加载预训练模型和分词器。\n",
    "#输入一段文本。\n",
    "#使用分词器对输入文本进行编码，并返回输入ID。\n",
    "#使用预训练模型生成文本，控制输出token的字符个数。\n",
    "#将生成的输出解码为文本。\n",
    "#输出结果。\n",
    "\n",
    "#实现原理：\n",
    "#这段代码的主要目的是使用预训练的GPT-2模型来生成文本。下面是代码的详细解释：\n",
    "\n",
    "#导入所需的库：首先，我们需要导入torch和transformers库。torch是一个用于构建（神经网络）的开源库，而transformers库则提供了一些（预训练）的模型和分词器，可以方便我们快速实现文本生成任务。\n",
    "\n",
    "#设置预训练模型名称：我们将预训练模型的名称设置为\"gpt2\"，这是一个（基于）Transformer架构的预训练语言模型，由Google开发。\n",
    "\n",
    "#加载预训练模型和分词器：接下来，我们使用GPT2Tokenizer.from_pretrained()方法加载预训练模型和分词器。这个方法会（根据给定的模型名称）加载对应的模型和分词器。\n",
    "\n",
    "#输入文本：然后，我们输入一段文本，这段文本将作为（模型的输入）。\n",
    "\n",
    "#（编码）输入文本：接下来，我们使用【分词器的encode()方法】对输入文本进行编码。这个方法会将文本（转换为模型可以理解）的形式，即输入ID。\n",
    "\n",
    "#生成文本：然后，我们使用模型的generate()方法生成文本。这个方法会（根据输入ID）生成一段新的文本。在这个方法中，我们还设置了max_length参数，用于控制生成文本的长度（也就是token）。\n",
    "\n",
    "#解码输出文本：最后，我们使用【分词器的decode()方法】将生成的输出文本（解码为人类可读）的形式。在解码过程中，我们使用了skip_special_tokens=True参数，以忽略特殊的令牌（如CLS、SEP等）。\n",
    "\n",
    "#输出结果：最后，我们打印出生成的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'output_at'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-28d82b63813f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# 使用模型的输出层获取最可能的token序列\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpredicted_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_at\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 根据token序列重新构建文本\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'output_at'"
     ]
    }
   ],
   "source": [
    "#迭代版本1：解决输出文本后面内容重复的情况，\n",
    "# 最简陋的一个语言模型，采用的是transformers和torch库\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# 输入文本\n",
    "text = \"the NBA is a great place to be. It's a great place to be.\"    #在此输入更长的文本，shift+enter运行代码\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 生成文本\n",
    "output = model.generate(input_ids, max_length=50, output_scores=True)  # 添加output_scores=True\n",
    "\n",
    "# 使用模型的输出层获取最可能的token序列\n",
    "predicted_tokens = output.output_at(1).logits.argmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "# 根据token序列重新构建文本\n",
    "decoded_output = ' '.join([tokenizer.decode(token_id, skip_special_tokens=True) for token_id in predicted_tokens])\n",
    "\n",
    "# 输出结果\n",
    "print(decoded_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以上的文本生成代码思路是采用openai已经训练好的模型GPT2直接进行的文本生成功能，并没有进行模型的训练\n",
    "##下面我们如果要训练自己的文本生成模型，大概思路是这样：以下是一个简单的基于PyTorch和Transformers库的文本生成模型训练代码示例：\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# 定义数据集类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 定义模型类\n",
    "class TextGenerationModel(nn.Module):\n",
    "    def __init__(self, n_vocab):\n",
    "        super(TextGenerationModel, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.fc(pooled_output)\n",
    "        return output\n",
    "\n",
    "# 准备数据\n",
    "texts = [\"Hello, I am a computer program.\", \"I love to play football.\", \"Today is a beautiful day.\"]\n",
    "labels = [0, 1, 2]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 100\n",
    "batch_size = 4\n",
    "n_vocab = len(tokenizer.vocab) + 1  # 加1是因为标签从0开始\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "model = TextGenerationModel(n_vocab)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):  # 训练10轮\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/10], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item()}')\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained('./text_gen_model')\n",
    "\n",
    "#这段代码首先定义了一个数据集类TextDataset和一个模型类TextGenerationModel，然后准备了一些简单的文本数据和对应的标签。接着，我们使用这些数据来训练一个基于BERT的文本生成模型。最后，我们将训练好的模型保存到本地。\n",
    "#这些数据集类和模型类是PyTorch和Transformers库提供的，我们可以直接使用它们来构建文本生成模型。\n",
    "\n",
    "#- `TextDataset`类是一个自定义的数据集类，用于加载文本数据和对应的标签，并进行必要的预处理。它继承自PyTorch的`Dataset`类，并实现了`__len__`和`__getitem__`方法。在`__getitem__`方法中，我们使用BERT的tokenizer对输入的文本进行编码，并将编码后的结果返回为一个字典，包含\"text\"、\"input_ids\"、\"attention_mask\"和\"label\"四个字段。\n",
    "#- `BertTokenizer`类是BERT的tokenizer，用于将文本转换为模型可以理解的形式。它提供了一些方法，如`encode_plus`，可以同时进行词法分析和编码，并返回token的类型和位置信息。\n",
    "#- `BertForSequenceClassification`类是基于BERT的序列分类模型，用于文本生成任务。它继承了PyTorch的`nn.Module`类，并实现了前向传播方法。在这个例子中，我们使用了预训练的BERT模型，并在其基础上添加了一个全连接层来进行分类。\n",
    "\n",
    "#在使用这些类时，我们需要先实例化它们，然后按照一定的顺序组合起来构建我们的文本生成模型。例如，我们可以先创建一个`BertTokenizer`对象，然后使用它对输入的文本进行编码，得到编码后的结果。接着，我们可以将编码后的结果输入到一个`BertForSequenceClassification`对象中，得到模型的输出。最后，我们可以使用损失函数来计算模型的损失值，并通过反向传播算法来更新模型的参数，从而优化模型的性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
